import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from typing import Annotated, TypedDict
from langgraph.graph.message import add_messages
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from tavily import TavilyClient

load_dotenv()


def pretty_print(message):
    """æ ¼å¼åŒ–æ‰“å°æ¶ˆæ¯å†…å®¹"""
    if hasattr(message, "content"):
        content = message.content
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            # å¤„ç†å¤šéƒ¨åˆ†å†…å®¹
            text_parts = []
            for item in content:
                if isinstance(item, dict):
                    if item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                    elif item.get("type") == "tool_use":
                        text_parts.append(
                            f"\n[å·¥å…·è°ƒç”¨: {item.get('name', 'unknown')}]\n"
                        )
                else:
                    text_parts.append(str(item))
            return "".join(text_parts)
        else:
            return str(content)
    return str(message)


llm = ChatOpenAI(
    model="deepseek-chat",
    temperature=0.7,
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url=os.getenv("DEEPSEEK_BASE_URL"),
    max_tokens=8192,
)
# åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))


class SearchState(TypedDict):
    messages: Annotated[list, add_messages]
    user_query: str  # ç»è¿‡LLMç†è§£åçš„ç”¨æˆ·éœ€æ±‚æ€»ç»“
    search_query: str  # ä¼˜åŒ–åç”¨äºTavily APIçš„æœç´¢æŸ¥è¯¢
    search_results: str  # Tavilyæœç´¢è¿”å›çš„ç»“æœ
    final_answer: str  # æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆ
    step: str  # æ ‡è®°å½“å‰æ­¥éª¤


def understand_query_node(state: SearchState) -> dict:
    user_message = state["messages"][-1].content
    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"
è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ
2. ç”Ÿæˆæœ€é€‚åˆæœç´¢å¼•æ“çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼‰

æ ¼å¼ï¼š
ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""
    response = llm.invoke([SystemMessage(content=understand_prompt)])
    response_text = response.content

    # è§£æLLMçš„è¾“å‡ºï¼Œæå–æœç´¢å…³é”®è¯
    search_query = user_message  # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢
    if "æœç´¢è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢è¯ï¼š")[1].strip()

    return {
        "user_query": response_text,
        "search_query": search_query,
        "step": "understood",
        "messages": [AIMessage(content=f"æˆ‘å°†ä¸ºæ‚¨æœç´¢ï¼š{search_query}")],
    }


def tavily_search_node(state: SearchState) -> dict:
    search_query = state["search_query"]
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢: {search_query}")
        response = tavily_client.search(
            query=search_query, search_depth="basic", max_results=5, include_answer=True
        )
        # ... (å¤„ç†å’Œæ ¼å¼åŒ–æœç´¢ç»“æœ) ...
        search_results = response.content
        return {
            "search_results": search_results,
            "step": "searched",
            "messages": [AIMessage(content="âœ… æœç´¢å®Œæˆï¼æ­£åœ¨æ•´ç†ç­”æ¡ˆ...")],
        }
    except Exception as e:
        # ... (å¤„ç†é”™è¯¯) ...
        return {
            "search_results": f"æœç´¢å¤±è´¥ï¼š{e}",
            "step": "search_failed",
            "messages": [AIMessage(content="âŒ æœç´¢é‡åˆ°é—®é¢˜...")],
        }


def generate_answer_node(state: SearchState) -> dict:
    """æ­¥éª¤3ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆæµå¼è¾“å‡ºï¼‰"""
    if state["step"] == "searched":
        # å¦‚æœæœç´¢å¤±è´¥ï¼Œæ‰§è¡Œå›é€€ç­–ç•¥ï¼ŒåŸºäºLLMè‡ªèº«çŸ¥è¯†å›ç­”
        fallback_prompt = f"æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\nç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}"
        messages = [SystemMessage(content=fallback_prompt)]
    else:
        # æœç´¢æˆåŠŸï¼ŒåŸºäºæœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        answer_prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å®Œæ•´ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š
ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}
æœç´¢ç»“æœï¼š\n{state['search_results']}
è¯·ç»¼åˆæœç´¢ç»“æœï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”..."""
        messages = [SystemMessage(content=answer_prompt)]

    # ä½¿ç”¨æµå¼è¾“å‡ºï¼Œå®æ—¶æ‰“å°å¹¶ç´¯ç§¯å†…å®¹
    full_content = ""
    print()  # æ¢è¡Œï¼Œå‡†å¤‡è¾“å‡ºç­”æ¡ˆ
    for chunk in llm.stream(messages):
        if hasattr(chunk, "content") and chunk.content:
            content = chunk.content
            full_content += content
            # å®æ—¶æ‰“å°æµå¼è¾“å‡º
            print(content, end="", flush=True)

    return {
        "final_answer": full_content,
        "step": "completed",
        "messages": [AIMessage(content=full_content)],
    }


def create_search_assistant():
    workflow = StateGraph(SearchState)

    # æ·»åŠ ç»“ç‚¹
    workflow.add_node("understand", understand_query_node)
    workflow.add_node("search", tavily_search_node)
    workflow.add_node("answer", generate_answer_node)

    # æ·»åŠ è¾¹
    workflow.add_edge(START, "understand")
    workflow.add_edge("understand", "search")
    workflow.add_edge("search", "answer")
    workflow.add_edge("answer", END)

    memory = InMemorySaver()
    app = workflow.compile(checkpointer=memory)
    return app


if __name__ == "__main__":
    assistant = create_search_assistant()
    user_query = "è¿™å‘¨å…­æˆ‘è¦å»å—äº¬ï¼Œå¤©æ°”æ€ä¹ˆæ ·ï¼Ÿæœ‰åˆé€‚çš„æ™¯ç‚¹å—"
    # Checkpointer requires thread_id in config
    config = {"configurable": {"thread_id": "test-thread-1"}}

    print("=" * 60)
    print(f"ç”¨æˆ·é—®é¢˜: {user_query}")
    print("=" * 60)
    print("\nå¼€å§‹å¤„ç†...\n")

    # æµå¼è¾“å‡º - ä½¿ç”¨ "values" æ¨¡å¼è·å–çŠ¶æ€æ›´æ–°
    result = assistant.stream(
        {"messages": [HumanMessage(content=user_query)]},
        config=config,
        stream_mode="values",
    )

    last_step = None
    for s in result:
        # æ˜¾ç¤ºæ­¥éª¤å˜åŒ–
        if "step" in s and s["step"] != last_step:
            step = s["step"]
            if step == "understood":
                print("\nğŸ“ [æ­¥éª¤ 1/3] ç†è§£ç”¨æˆ·éœ€æ±‚...")
            elif step == "searched":
                print("\nğŸ” [æ­¥éª¤ 2/3] æœç´¢ä¿¡æ¯...")
            elif step == "completed":
                print("\nğŸ’¬ [æ­¥éª¤ 3/3] ç”Ÿæˆç­”æ¡ˆ...")
            last_step = step

        # æ³¨æ„ï¼šLLM çš„æµå¼è¾“å‡ºå·²ç»åœ¨ generate_answer_node ä¸­å®æ—¶æ‰“å°äº†
        # è¿™é‡Œåªéœ€è¦ç­‰å¾…æµå®Œæˆå³å¯

    print("\n\n" + "=" * 60)
    print("âœ… å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print()
